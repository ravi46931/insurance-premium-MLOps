{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Insurance Premium Prediction Application Overview This is a machine learning application designed for predicting insurance premiums. The project leverages a variety of tools and frameworks to streamline data management, experiment tracking, and model deployment. \ud83d\udee0\ufe0f Tools Utilized DVC (Data Version Control) : Used for managing and versioning data pipeline. Git : Version control system for tracking code changes. MLflow : Used for tracking the model training and model evaluation. GitHub Actions Server : Used for continuous integration and deployment. Dagshub : Facilitates MLflow experiment tracking and DVC data pipeline. \ud83d\udee2\ufe0f Machine Learning Pipeline Data Ingestion \ud83d\udce5 The application ingests insurance premium data from the data/insurance.csv data path and saves it into artifacts/DataIngestionArtifacts . Data Transformation \ud83d\udd27 Data undergoes transformation to prepare it for model training. Transformed data and preprocessing artifacts are saved into artifacts/DataTransformationArtifacts . Preprocessors are also stored in models/ . Model Training \ud83e\udd16 Multiple machine learning models are trained: Linear Regression, Ridge Regression, Lasso Regression, Polynomial Regression, Random Forest, Gradient Boosting, XGBoost, LightGBM, Catboost. The top 4 performing models based on training metrics are selected. Both models and associated metrics are saved into artifacts/ModelTrainerArtifacts . MLflow is used to track model parameters and metrics throughout this process. Model Evaluation \ud83d\udcca The best-performing model on test data is selected and saved into artifacts/ModelEvaluationArtifacts and models/ . Model evaluation metrics are tracked using MLflow. Streamlit App Development \ud83d\udcbb A Streamlit application is developed to allow users to input data and receive predictions from the trained model. Model Deployment \ud83d\ude80 The model is deployend on the AWS EC2 using Docker and Github Action Server . \ud83d\udccb Model tracking with MLFlow \ud83d\udd87\ufe0f Data pipeline tracking with DVC \ud83d\udcc1 Directory Structure \ud83d\udcc2.github/ \u2514\u2500\u2500 \ud83d\udcc2workflows/ \u2514\u2500\u2500 main.yaml \ud83d\udcc2docs/ \u251c\u2500\u2500 \ud83d\udcc2docs/ \u2502 \u251c\u2500\u2500 index.md \u2502 \u2514\u2500\u2500 getting-started.md \u251c\u2500\u2500 mkdocs.yml \u2514\u2500\u2500 README.md \ud83d\udcc2src/ \u251c\u2500\u2500 init.py \u251c\u2500\u2500 \ud83d\udcc2components/ \u2502 \u251c\u2500\u2500 init.py \u2502 \u251c\u2500\u2500 data_ingestion.py \u2502 \u251c\u2500\u2500 data_transformation.py \u2502 \u251c\u2500\u2500 model_trainer.py \u2502 \u2514\u2500\u2500 model_evaluation.py \u251c\u2500\u2500 \ud83d\udcc2constants/ \u2502 \u2514\u2500\u2500 init.py \u251c\u2500\u2500 \ud83d\udcc2entity/ \u2502 \u251c\u2500\u2500 init.py \u2502 \u251c\u2500\u2500 config_entity.py \u2502 \u2514\u2500\u2500 artifact_entity.py \u251c\u2500\u2500 \ud83d\udcc2pipeline/ \u2502 \u251c\u2500\u2500 init.py \u2502 \u251c\u2500\u2500 training_pipeline.py \u2502 \u2514\u2500\u2500 prediction_pipeline.py \u251c\u2500\u2500 \ud83d\udcc2utils/ \u2502 \u251c\u2500\u2500 init.py \u2502 \u2514\u2500\u2500 utils.py \u251c\u2500\u2500 \ud83d\udcc2logger/ \u2502 \u2514\u2500\u2500 init.py \u2514\u2500\u2500 \ud83d\udcc2exception/ \u2514\u2500\u2500 init.py \ud83d\udcc2data/ \u2514\u2500\u2500 insurance.csv \ud83d\udcc2experiment/ \u2514\u2500\u2500 experiments.ipynb requirements.txt requirements_app.txt setup.py app.py main.py README.md implement.md .gitignore template.py prediction.py init_setup.ps1 dvc.yaml Dockerfile demo.py config.json .dockerignore .dvcignore \ud83d\udcc8 Models Linear Regression Ridge Regression Lasso Regression Polynomial Regression Random Forest Gradient Boosting XGBoost LightGBM Catboost \ud83d\udda5\ufe0f Installation \ud83d\udee0\ufe0f Requirements: Python 3.10 mkdocs dvc numpy pandas colorama mlflow==2.2.2 dagshub scikit-learn xgboost lightgbm catboost streamlit \u2699\ufe0f Setup To reproduce the model and run the application: Clone the repository: git clone <repository_url> cd <repository_name> Set up the virtual environment and install the requirements: ./init_setup.ps1 Execute the whole pipeline: python main.py Now run the streamlit app. \ud83c\udfaf Inference demo Run the Streamlit app: streamlit run app.py 2. Enter the input values and get prediction Contributors \ud83d\udc68\ud83c\udffc\u200d\ud83d\udcbb Ravi Kumar","title":"Home"},{"location":"#insurance-premium-prediction-application","text":"","title":"Insurance Premium Prediction Application"},{"location":"#overview","text":"This is a machine learning application designed for predicting insurance premiums. The project leverages a variety of tools and frameworks to streamline data management, experiment tracking, and model deployment.","title":"Overview"},{"location":"#tools-utilized","text":"DVC (Data Version Control) : Used for managing and versioning data pipeline. Git : Version control system for tracking code changes. MLflow : Used for tracking the model training and model evaluation. GitHub Actions Server : Used for continuous integration and deployment. Dagshub : Facilitates MLflow experiment tracking and DVC data pipeline.","title":"\ud83d\udee0\ufe0f Tools Utilized"},{"location":"#machine-learning-pipeline","text":"","title":"\ud83d\udee2\ufe0f Machine Learning Pipeline"},{"location":"#data-ingestion","text":"The application ingests insurance premium data from the data/insurance.csv data path and saves it into artifacts/DataIngestionArtifacts .","title":"Data Ingestion \ud83d\udce5"},{"location":"#data-transformation","text":"Data undergoes transformation to prepare it for model training. Transformed data and preprocessing artifacts are saved into artifacts/DataTransformationArtifacts . Preprocessors are also stored in models/ .","title":"Data Transformation \ud83d\udd27"},{"location":"#model-training","text":"Multiple machine learning models are trained: Linear Regression, Ridge Regression, Lasso Regression, Polynomial Regression, Random Forest, Gradient Boosting, XGBoost, LightGBM, Catboost. The top 4 performing models based on training metrics are selected. Both models and associated metrics are saved into artifacts/ModelTrainerArtifacts . MLflow is used to track model parameters and metrics throughout this process.","title":"Model Training \ud83e\udd16"},{"location":"#model-evaluation","text":"The best-performing model on test data is selected and saved into artifacts/ModelEvaluationArtifacts and models/ . Model evaluation metrics are tracked using MLflow.","title":"Model Evaluation \ud83d\udcca"},{"location":"#streamlit-app-development","text":"A Streamlit application is developed to allow users to input data and receive predictions from the trained model.","title":"Streamlit App Development \ud83d\udcbb"},{"location":"#model-deployment","text":"The model is deployend on the AWS EC2 using Docker and Github Action Server .","title":"Model Deployment \ud83d\ude80"},{"location":"#model-tracking-with-mlflow","text":"","title":"\ud83d\udccb Model tracking with MLFlow"},{"location":"#data-pipeline-tracking-with-dvc","text":"","title":"\ud83d\udd87\ufe0f Data pipeline tracking with DVC"},{"location":"#directory-structure","text":"\ud83d\udcc2.github/ \u2514\u2500\u2500 \ud83d\udcc2workflows/ \u2514\u2500\u2500 main.yaml \ud83d\udcc2docs/ \u251c\u2500\u2500 \ud83d\udcc2docs/ \u2502 \u251c\u2500\u2500 index.md \u2502 \u2514\u2500\u2500 getting-started.md \u251c\u2500\u2500 mkdocs.yml \u2514\u2500\u2500 README.md \ud83d\udcc2src/ \u251c\u2500\u2500 init.py \u251c\u2500\u2500 \ud83d\udcc2components/ \u2502 \u251c\u2500\u2500 init.py \u2502 \u251c\u2500\u2500 data_ingestion.py \u2502 \u251c\u2500\u2500 data_transformation.py \u2502 \u251c\u2500\u2500 model_trainer.py \u2502 \u2514\u2500\u2500 model_evaluation.py \u251c\u2500\u2500 \ud83d\udcc2constants/ \u2502 \u2514\u2500\u2500 init.py \u251c\u2500\u2500 \ud83d\udcc2entity/ \u2502 \u251c\u2500\u2500 init.py \u2502 \u251c\u2500\u2500 config_entity.py \u2502 \u2514\u2500\u2500 artifact_entity.py \u251c\u2500\u2500 \ud83d\udcc2pipeline/ \u2502 \u251c\u2500\u2500 init.py \u2502 \u251c\u2500\u2500 training_pipeline.py \u2502 \u2514\u2500\u2500 prediction_pipeline.py \u251c\u2500\u2500 \ud83d\udcc2utils/ \u2502 \u251c\u2500\u2500 init.py \u2502 \u2514\u2500\u2500 utils.py \u251c\u2500\u2500 \ud83d\udcc2logger/ \u2502 \u2514\u2500\u2500 init.py \u2514\u2500\u2500 \ud83d\udcc2exception/ \u2514\u2500\u2500 init.py \ud83d\udcc2data/ \u2514\u2500\u2500 insurance.csv \ud83d\udcc2experiment/ \u2514\u2500\u2500 experiments.ipynb requirements.txt requirements_app.txt setup.py app.py main.py README.md implement.md .gitignore template.py prediction.py init_setup.ps1 dvc.yaml Dockerfile demo.py config.json .dockerignore .dvcignore","title":"\ud83d\udcc1 Directory Structure"},{"location":"#models","text":"Linear Regression Ridge Regression Lasso Regression Polynomial Regression Random Forest Gradient Boosting XGBoost LightGBM Catboost","title":"\ud83d\udcc8 Models"},{"location":"#installation","text":"","title":"\ud83d\udda5\ufe0f Installation"},{"location":"#requirements","text":"Python 3.10 mkdocs dvc numpy pandas colorama mlflow==2.2.2 dagshub scikit-learn xgboost lightgbm catboost streamlit","title":"\ud83d\udee0\ufe0f Requirements:"},{"location":"#setup","text":"To reproduce the model and run the application: Clone the repository: git clone <repository_url> cd <repository_name> Set up the virtual environment and install the requirements: ./init_setup.ps1 Execute the whole pipeline: python main.py Now run the streamlit app.","title":"\u2699\ufe0f Setup"},{"location":"#inference-demo","text":"Run the Streamlit app: streamlit run app.py 2. Enter the input values and get prediction","title":"\ud83c\udfaf Inference demo"},{"location":"#contributors","text":"Ravi Kumar","title":"Contributors \ud83d\udc68\ud83c\udffc\u200d\ud83d\udcbb"},{"location":"getting-started/","text":"","title":"About"}]}